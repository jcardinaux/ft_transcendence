input {
  file {
    path => "/logs/server.log"
    start_position => "beginning"
    #sincedb_path => "/usr/share/logstash/data/sincedb_server"
    sincedb_path => "/dev/null"
    codec => json { 
      charset => "UTF-8"
      target => "log_data"  # <- Resolves ECS warning
    }
    tags => ["fastify"]
    type => "backend"
    ignore_older => 0
    
    # Optimizations for real-time monitoring
    stat_interval => 1
    discover_interval => 5
    close_older => 1      # Closes inactive files after 1 hour
    max_open_files => 4096
  }

  file {
    path => "/logs/client.log"
    start_position => "beginning"
    #sincedb_path => "/usr/share/logstash/data/sincedb_client"
    sincedb_path => "/dev/null"
    codec => json { 
      charset => "UTF-8"
      target => "log_data"  # <- Resolves ECS warning
    }
    tags => ["client"]
    type => "web-client"
    ignore_older => 0
    
    # Same optimizations
    stat_interval => 1
    discover_interval => 5
    close_older => 1
    max_open_files => 4096
  }
}

filter {
  # Extract fields from log_data if present
  if [log_data] {
    mutate {
      add_field => { 
        "level" => "%{[log_data][level]}"
        "time" => "%{[log_data][time]}"
        "msg" => "%{[log_data][msg]}"
        "pid" => "%{[log_data][pid]}"
        "hostname" => "%{[log_data][hostname]}"
      }
    }
    
    # Copy reqId if present (for backend)
    if [log_data][reqId] {
      mutate { add_field => { "reqId" => "%{[log_data][reqId]}" } }
    }
    
    # Copy context if present (for client)
    if [log_data][context] {
      mutate { add_field => { "context" => "%{[log_data][context]}" } }
    }
  }

  # Timestamp handling
  if [type] == "backend" and [reqId] {
    date {
      match => ["time", "UNIX_MS"]
      target => "@timestamp"
    }
  } else {
    date {
      match => ["time", "UNIX_MS"]
      target => "@timestamp"
    }
  }

  # Add service field
  if [type] == "web-client" {
    mutate { add_field => { "service" => "ft_client" } }
  } else {
    mutate { add_field => { "service" => "ft_backend" } }
  }

  # Fingerprint for upsert in ES
  fingerprint {
    source => ["path", "host", "msg", "time"]
    target => "[@metadata][fingerprint]"
    method => "SHA256"
  }

  # Final cleanup
  mutate {
    remove_field => ["pid", "hostname", "time", "event", "log_data"]
    rename => { "level" => "log_level" }
  }
}

output {
  elasticsearch {
    hosts => ["https://elasticsearch:9200"]
    index => "fttranscendence-logs-%{+YYYY.MM.dd}"
    user => "${ES_USER}"
    password => "${ES_PASS}"
    ssl => true
    ssl_certificate_verification => false
    cacert => "/shared-certs/ca/ca-cert.pem"
    document_id => "%{[@metadata][fingerprint]}"
  }
  
  # stdout only for debugging -> remove in production
  stdout { codec => rubydebug }
}

# # If you need to reduce "noise" on your stdout (because it outputs the same events twice when restarting), just use /dev/null for debugging and then switch to a real sincedb_path => "/usr/share/logstash/data/sincedb_..." in production.